<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Publications | Max W.F. Ku </title> <meta name="author" content="Max W.F. Ku"> <meta name="description" content="publications in reversed chronological order."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon.png?3826822d2e00e12056c49e01e68b7416"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://kuwingfung.github.io/publications/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Max W.F. Ku </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/misc/">Miscellaneous </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description">publications in reversed chronological order.</p> </header> <article> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ACL 2025</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/tha-480.webp 480w,/assets/img/publication_preview/tha-800.webp 800w,/assets/img/publication_preview/tha-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/tha.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="tha.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ku2025theoremexplainagentmultimodalexplanationsllm" class="col-sm-8"> <div class="title">TheoremExplainAgent: Towards Multimodal Explanations for LLM Theorem Understanding</div> <div class="author"> <em>Max Ku*</em>, Thomas Chong*, Jonathan Leung, Krish Shah, Alvin Yu, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Wenhu Chen' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '5'); ">1 more author</span> </div> <div class="periodical"> 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2502.19400" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/TIGER-AI-Lab/TheoremExplainAgent" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://tiger-ai-lab.github.io/TheoremExplainAgent" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">ku2025theoremexplainagentmultimodalexplanationsllm</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{TheoremExplainAgent: Towards Multimodal Explanations for LLM Theorem Understanding}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{The 63rd Annual Meeting of the Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ku*, Max and Chong*, Thomas and Leung, Jonathan and Shah, Krish and Yu, Alvin and Chen, Wenhu}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="nv">false</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">EMNLP 2024</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/videoscore-480.webp 480w,/assets/img/publication_preview/videoscore-800.webp 800w,/assets/img/publication_preview/videoscore-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/videoscore.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="videoscore.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="he2024videoscorebuildingautomaticmetrics" class="col-sm-8"> <div class="title">VideoScore: Building Automatic Metrics to Simulate Fine-grained Human Feedback for Video Generation</div> <div class="author"> Xuan He, Dongfu Jiang, Ge Zhang, <em>Max Ku</em>, Achint Soni, and <span class="more-authors" title="click to view 14 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '14 more authors' ? 'Sherman Siu, Haonan Chen, Abhranil Chandra, Ziyan Jiang, Aaran Arulraj, Kai Wang, Quy Duc Do, Yuansheng Ni, Bohan Lyu, Yaswanth Narsupalli, Rongqi Fan, Zhiheng Lyu, Yuchen Lin, Wenhu Chen' : '14 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '5'); ">14 more authors</span> </div> <div class="periodical"> 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2406.15252" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/TIGER-AI-Lab/VideoScore" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://tiger-ai-lab.github.io/VideoScore/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">he2024videoscorebuildingautomaticmetrics</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{VideoScore: Building Automatic Metrics to Simulate Fine-grained Human Feedback for Video Generation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{He, Xuan and Jiang, Dongfu and Zhang, Ge and Ku, Max and Soni, Achint and Siu, Sherman and Chen, Haonan and Chandra, Abhranil and Jiang, Ziyan and Arulraj, Aaran and Wang, Kai and Do, Quy Duc and Ni, Yuansheng and Lyu, Bohan and Narsupalli, Yaswanth and Fan, Rongqi and Lyu, Zhiheng and Lin, Yuchen and Chen, Wenhu}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Empirical Methods in Natural Language Processing}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">false</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="nv">false</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS 2024</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/mmlu_pro-480.webp 480w,/assets/img/publication_preview/mmlu_pro-800.webp 800w,/assets/img/publication_preview/mmlu_pro-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/mmlu_pro.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="mmlu_pro.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wang2024mmlupro" class="col-sm-8"> <div class="title">MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark</div> <div class="author"> Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, and <span class="more-authors" title="click to view 12 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '12 more authors' ? 'Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, Wenhu Chen' : '12 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '5'); ">12 more authors</span> </div> <div class="periodical"> 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2406.01574" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/TIGER-AI-Lab/MMLU-Pro" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">wang2024mmlupro</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wang, Yubo and Ma, Xueguang and Zhang, Ge and Ni, Yuansheng and Chandra, Abhranil and Guo, Shiguang and Ren, Weiming and Arulraj, Aaran and He, Xuan and Jiang, Ziyan and Li, Tianle and Ku, Max and Wang, Kai and Zhuang, Alex and Fan, Rongqi and Yue, Xiang and Chen, Wenhu}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Conference on Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">false</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="nv">false</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS 2024</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/genai_arena-480.webp 480w,/assets/img/publication_preview/genai_arena-800.webp 800w,/assets/img/publication_preview/genai_arena-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/genai_arena.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="genai_arena.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="jiang2024genai" class="col-sm-8"> <div class="title">GenAI Arena: An Open Evaluation Platform for Generative Models</div> <div class="author"> Dongfu Jiang*, <em>Max Ku*</em>, Tianle Li*, Yuansheng Ni, Shizhuo Sun, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Rongqi Fan, Wenhu Chen' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '5'); ">2 more authors</span> </div> <div class="periodical"> 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2406.04485v1" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://huggingface.co/spaces/TIGER-Lab/GenAI-Arena/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">jiang2024genai</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{GenAI Arena: An Open Evaluation Platform for Generative Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jiang*, Dongfu and Ku*, Max and Li*, Tianle and Ni, Yuansheng and Sun, Shizhuo and Fan, Rongqi and Chen, Wenhu}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Conference on Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="nv">false</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">TMLR 2024</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/mantis_preview-480.webp 480w,/assets/img/publication_preview/mantis_preview-800.webp 800w,/assets/img/publication_preview/mantis_preview-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/mantis_preview.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="mantis_preview.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="jiang2024mantis" class="col-sm-8"> <div class="title">MANTIS: Interleaved Multi-Image Instruction Tuning</div> <div class="author"> Dongfu Jiang, Xuan He, Huaye Zeng, Cong Wei, <em>Max Ku</em>, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Qian Liu, Wenhu Chen' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '5'); ">2 more authors</span> </div> <div class="periodical"> <em>Transactions on Machine Learning Research</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2405.01483" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/TIGER-AI-Lab/Mantis" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://tiger-ai-lab.github.io/Mantis/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">jiang2024mantis</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{MANTIS: Interleaved Multi-Image Instruction Tuning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jiang, Dongfu and He, Xuan and Zeng, Huaye and Wei, Cong and Ku, Max and Liu, Qian and Chen, Wenhu}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Transactions on Machine Learning Research}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">false</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="nv">false</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">TMLR 2024</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/anyv2v-480.webp 480w,/assets/img/publication_preview/anyv2v-800.webp 800w,/assets/img/publication_preview/anyv2v-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/anyv2v.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="anyv2v.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ku2024anyv2v" class="col-sm-8"> <div class="title">AnyV2V: A Tuning-Free Framework For Any Video-to-Video Editing Tasks</div> <div class="author"> <em>Max Ku*</em>, Cong Wei*, Weiming Ren*, Harry Yang, and Wenhu Chen </div> <div class="periodical"> <em>Transactions on Machine Learning Research</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2403.14468" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/TIGER-AI-Lab/AnyV2V" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://tiger-ai-lab.github.io/AnyV2V/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ku2024anyv2v</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{AnyV2V: A Tuning-Free Framework For Any Video-to-Video Editing Tasks}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ku*, Max and Wei*, Cong and Ren*, Weiming and Yang, Harry and Chen, Wenhu}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Transactions on Machine Learning Research}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="nv">false</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ACL 2024</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/viescore-480.webp 480w,/assets/img/publication_preview/viescore-800.webp 800w,/assets/img/publication_preview/viescore-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/viescore.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="viescore.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Ku2023VIEScoreTE" class="col-sm-8"> <div class="title">VIEScore: Towards Explainable Metrics for Conditional Image Synthesis Evaluation</div> <div class="author"> <em>Max Ku</em>, Dongfu Jiang, Cong Wei, Xiang Yue, and Wenhu Chen </div> <div class="periodical"> <em>In The 62nd Annual Meeting of the Association for Computational Linguistics</em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2312.14867" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/TIGER-AI-Lab/VIEScore" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://tiger-ai-lab.github.io/VIEScore/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Ku2023VIEScoreTE</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{VIEScore: Towards Explainable Metrics for Conditional Image Synthesis Evaluation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ku, Max and Jiang, Dongfu and Wei, Cong and Yue, Xiang and Chen, Wenhu}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{The 62nd Annual Meeting of the Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="nv">false</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICLR 2024</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/imagenhub-480.webp 480w,/assets/img/publication_preview/imagenhub-800.webp 800w,/assets/img/publication_preview/imagenhub-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/imagenhub.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="imagenhub.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ku2024imagenhub" class="col-sm-8"> <div class="title">ImagenHub: Standardizing the evaluation of conditional image generation models</div> <div class="author"> <em>Max Ku</em>, Tianle Li, Kai Zhang, Yujie Lu, Xingyu Fu, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Wenwen Zhuang, Wenhu Chen' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '5'); ">2 more authors</span> </div> <div class="periodical"> <em>In The Twelfth International Conference on Learning Representations</em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2310.01596" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/TIGER-AI-Lab/ImagenHub" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://tiger-ai-lab.github.io/ImagenHub/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ku2024imagenhub</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ImagenHub: Standardizing the evaluation of conditional image generation models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ku, Max and Li, Tianle and Zhang, Kai and Lu, Yujie and Fu, Xingyu and Zhuang, Wenwen and Chen, Wenhu}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{The Twelfth International Conference on Learning Representations}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">twitter</span> <span class="p">=</span> <span class="s">{https://twitter.com/vinesmsuic/status/1717564355212951701}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="nv">false</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">TMLR 2023</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/dreamedit-480.webp 480w,/assets/img/publication_preview/dreamedit-800.webp 800w,/assets/img/publication_preview/dreamedit-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/dreamedit.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="dreamedit.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="li2023dreamedit" class="col-sm-8"> <div class="title">DreamEdit: Subject-driven Image Editing</div> <div class="author"> Tianle Li, <em>Max Ku*</em>, Cong Wei*, and Wenhu Chen </div> <div class="periodical"> <em>Transactions on Machine Learning Research</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2306.12624" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/DreamEditBenchTeam/DreamEdit" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://dreameditbenchteam.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">li2023dreamedit</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{DreamEdit: Subject-driven Image Editing}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Li, Tianle and Ku*, Max and Wei*, Cong and Chen, Wenhu}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Transactions on Machine Learning Research}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">false</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="nv">false</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">EMNLP 2023</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/theoremqa-480.webp 480w,/assets/img/publication_preview/theoremqa-800.webp 800w,/assets/img/publication_preview/theoremqa-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/theoremqa.jpeg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="theoremqa.jpeg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="chen2023theoremqa" class="col-sm-8"> <div class="title">TheoremQA: A Theorem-driven Question Answering dataset</div> <div class="author"> Wenhu Chen, Ming Yin, <em>Max Ku</em>, Yixin Wan, Xueguang Ma, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Jianyu Xu, Tony Xia, Xinyi Wang, Pan Lu' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '5'); ">4 more authors</span> </div> <div class="periodical"> <em>In Empirical Methods in Natural Language Processing</em> , 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2306.12624" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">chen2023theoremqa</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{TheoremQA: A Theorem-driven Question Answering dataset}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chen, Wenhu and Yin, Ming and Ku, Max and Wan, Yixin and Ma, Xueguang and Xu, Jianyu and Xia, Tony and Wang, Xinyi and Lu, Pan}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Empirical Methods in Natural Language Processing}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">false</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="nv">false</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICIP 2023</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/ipainter-480.webp 480w,/assets/img/publication_preview/ipainter-800.webp 800w,/assets/img/publication_preview/ipainter-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/ipainter.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="ipainter.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ku2023ipainter" class="col-sm-8"> <div class="title">Intelligent Painter: Picture Composition with Resampling Diffusion Model</div> <div class="author"> <em>Max Ku</em>, Wan-Chi Siu, Xi Cheng, and H. Anthony Chan </div> <div class="periodical"> <em>In International Conference on Image Processing</em> , 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2210.17106" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/vinesmsuic/ipainter-diffusion" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ku2023ipainter</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ku, Max and Siu, Wan-Chi and Cheng, Xi and Chan, H. Anthony}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2255-2259}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Intelligent Painter: Picture Composition with Resampling Diffusion Model}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICIP49359.2023.10222829}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Image Processing}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">false</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="nv">false</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Max W.F. Ku. ... Last updated: June 03, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let theme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===theme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"Publications",description:"publications in reversed chronological order.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"Projects",description:"A growing collection of my cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-miscellaneous",title:"Miscellaneous",description:"",section:"Navigation",handler:()=>{window.location.href="/misc/"}},{id:"post-paper-review-audio-visual-related-research-wip",title:"Paper Review - Audio-Visual Related Research (WIP)",description:"A survey on Audio-Visual Research. Encrypted with password.",section:"Posts",handler:()=>{window.open("http://vinesmsuic.github.io/paper-survey-av/","_blank")}},{id:"post-implementing-rag-for-code-library-documentation",title:"Implementing RAG for Code Library Documentation",description:"I tried to implement RAG for Code Library Documentation. This note help me to remind the important steps in setting up a RAG.",section:"Posts",handler:()=>{window.open("http://vinesmsuic.github.io/exp-rag/","_blank")}},{id:"post-evaluating-protein-transfer-learning-with-tape",title:"Evaluating Protein Transfer Learning with TAPE",description:"Tasks Assessing Protein Embeddings (TAPE) is a benchmark designed to evaluate semi-supervised learning on protein sequences.",section:"Posts",handler:()=>{window.open("http://vinesmsuic.github.io/paper-tape/","_blank")}},{id:"post-paper-review-modality-gap-and-alignment-in-multi-modal-contrastive-learning",title:"Paper Review - Modality Gap and Alignment in Multi-modal Contrastive Learning",description:"Contrastive learning is a popular self-supervised learning technique that has shown remarkable success in training deep neural networks. The core idea behind contrastive learning is to learn representations that are not only discriminative but also invariant to various transformations. This is achieved by contrasting positive and negative samples in the embedding space.",section:"Posts",handler:()=>{window.open("http://vinesmsuic.github.io/paper-cl-gap/","_blank")}},{id:"post-notes-on-score-based-generative-models",title:"Notes on Score-Based Generative Models",description:"My personal notes for studying diffusion models. Watching Dome&#39;s youtube video to learn.",section:"Posts",handler:()=>{window.open("http://vinesmsuic.github.io/paper-ddpm-score/","_blank")}},{id:"post-paper-review-pdae-disdiff-and-infodiffusion",title:"Paper Review - PDAE, DisDiff and InfoDiffusion",description:"Literature review on Unsupervised Representation Learning in diffusion models.",section:"Posts",handler:()=>{window.open("http://vinesmsuic.github.io/paper-disdiff/","_blank")}},{id:"post-paper-review-colorpeel",title:"Paper Review - ColorPeel",description:"An interesting paper from ECCV2024. It talks about the color and shape disentanglement on Text-to-Image models. The solution is simple yet effective.",section:"Posts",handler:()=>{window.open("http://vinesmsuic.github.io/paper-colorpeel/","_blank")}},{id:"post-paper-review-disentangled-contrastive-learning-on-graphs",title:"Paper Review - Disentangled Contrastive Learning on Graphs",description:"Revisit Contrastive learningContrastive learning is an instance-wise discriminative approach that aims at making",section:"Posts",handler:()=>{window.open("http://vinesmsuic.github.io/paper-dgcl/","_blank")}},{id:"post-paper-review-t-cell-receptor-meets-vae",title:"Paper Review - T-Cell Receptor meets VAE",description:"T-cell receptors (TCR) bind to certain antigens found on abnormal cells. We can perform TCR engineering with VAEs.",section:"Posts",handler:()=>{window.open("http://vinesmsuic.github.io/paper-tcr/","_blank")}},{id:"post-paper-review-evolutionary-scale-modeling",title:"Paper Review - Evolutionary Scale Modeling",description:"ESM-1 and ESM-2 refer to protein language models developed by Facebook AI Research under the project name Evolutionary Scale Modeling (ESM). These models are designed to understand and predict various properties of proteins by leveraging the principles of language modeling in a manner similar to how models like GPT-3 process natural language.",section:"Posts",handler:()=>{window.open("http://vinesmsuic.github.io/paper-esm/","_blank")}},{id:"post-paper-review-alphafold2-and-alphafold3",title:"Paper Review - AlphaFold2 and AlphaFold3",description:"Let&#39;s try to figure out whats inside AlphaFold! AlphaFold can accurately predict structures of biomolecular interactions.",section:"Posts",handler:()=>{window.open("http://vinesmsuic.github.io/paper-alphafold/","_blank")}},{id:"post-paper-review-proteindt",title:"Paper Review - ProteinDT",description:"ProteinDT is A Opensource Text-guided Protein Design Framework that uses Contrastive learning to align the two modalities.",section:"Posts",handler:()=>{window.open("http://vinesmsuic.github.io/paper-proteinDT/","_blank")}},{id:"post-paper-review-axial-transformer-and-msa-transformer",title:"Paper Review - Axial Transformer and MSA Transformer",description:"The Axial Transformer is aimed at managing the complexity of high-dimensional data like images and videos, while The MSA transformer is focused on biological sequences and their alignments.",section:"Posts",handler:()=>{window.open("http://vinesmsuic.github.io/paper-msa-trans/","_blank")}},{id:"post-paper-review-instantid",title:"Paper Review - InstantID",description:"Identity-Perserving Generation task refers to synthesizing an image that contains a desired person&#39;s identity. InstantID tackles this task with only one reference facial image. Moreover, InstantID does not require test-time training while utilizing the learned prior from diffusion-based foundation models like Stable Diffusion. Such methodology is highly efficient compared to previous methods that require fine-tuning on the foundation model. According to the paper, this work shows superior results in Identity-Perserving Generation, outperforming LoRA-based and IP-adapter-based methods qualitatively.",section:"Posts",handler:()=>{window.open("http://vinesmsuic.github.io/paper-instantid/","_blank")}},{id:"post-notes-on-video-diffusion-models",title:"Notes on Video Diffusion Models",description:"My handwritten notes on Video Diffusion models. Includes TokenFlow, Fairy, LaVie",section:"Posts",handler:()=>{window.open("http://vinesmsuic.github.io/paper-videodiff/","_blank")}},{id:"post-reflection-in-2023-encrypted",title:"Reflection in 2023 [Encrypted]",description:"Its been a while since my last blogging post. I put my collection of thoughts throughout 2023 here.",section:"Posts",handler:()=>{window.open("http://vinesmsuic.github.io/thoughts-2023-psy/","_blank")}},{id:"post-implementing-team-fortress-2-tf2-shader-in-c",title:"Implementing Team Fortress 2 (TF2) Shader in C++",description:"A small project that I did for my Computer Graphics course at uwaterloo.",section:"Posts",handler:()=>{window.open("http://vinesmsuic.github.io/notes-cg-project/","_blank")}},{id:"post-rbfn-kson-and-hopfield-network",title:"RBFN, KSON and Hopfield Network",description:"We cover Radial Basis Function Network (RBFN), Kohonen\u2019s Self-Organizing Network (KSON) and Hopfield Network.",section:"Posts",handler:()=>{window.open("http://vinesmsuic.github.io/notes-ece657-ch1234/","_blank")}},{id:"post-fuzzy-logic-systems",title:"Fuzzy Logic Systems",description:"Fuzzy logic was first developed by L.A. Zadeh in 1960\u2019s to extend conventional (binary) crisp logic to make it suitable to incorporate knowledge and mimic human-like approximate reasoning.",section:"Posts",handler:()=>{window.open("http://vinesmsuic.github.io/notes-ece657-ch567/","_blank")}},{id:"post-advanced-shading-and-camera-models-in-computer-graphics",title:"Advanced Shading and Camera Models in Computer Graphics",description:"Advanced Shading, Shadows and Reflections, Advanced Camera Models, Colors and stuffs. I just jot the keynotes here.",section:"Posts",handler:()=>{window.open("http://vinesmsuic.github.io/notes-cg-11/","_blank")}},{id:"news-joined-lt-a-href-quot-https-research-nvidia-com-labs-dir-quot-gt-nvidia-dir-lt-a-gt-as-an-intern-for-summer-2025",title:"Joined &lt;a href=&quot;https://research.nvidia.com/labs/dir/&quot;&gt;NVIDIA DIR&lt;/a&gt; as an intern for Summer 2025.",description:"",section:"News"},{id:"news-lt-a-href-quot-https-github-com-tiger-ai-lab-anyv2v-quot-gt-anyv2v-lt-a-gt-got-accepted-to-tmlr-2024",title:"&lt;a href=&quot;https://github.com/TIGER-AI-Lab/AnyV2V&quot;&gt;AnyV2V&lt;/a&gt; got accepted to TMLR 2024!",description:"",section:"News"},{id:"news-lt-a-href-quot-https-huggingface-co-spaces-tiger-lab-genai-arena-quot-gt-genaiarena-lt-a-gt-got-accepted-to-neurips-2024-see-you-in-vancouver",title:"&lt;a href=&quot;https://huggingface.co/spaces/TIGER-Lab/GenAI-Arena&quot;&gt;GenAIArena&lt;/a&gt; got accepted to NeurIPS 2024! See you in Vancouver!",description:"",section:"News"},{id:"news-lt-a-href-quot-https-tiger-ai-lab-github-io-theoremexplainagent-quot-gt-theoremexplainagent-lt-a-gt-got-accepted-to-acl-2025-main",title:"&lt;a href=&quot;https://tiger-ai-lab.github.io/TheoremExplainAgent/&quot;&gt;TheoremExplainAgent&lt;/a&gt; got accepted to ACL 2025 Main!",description:"",section:"News"},{id:"news-lt-a-href-quot-https-tiger-ai-lab-github-io-viescore-quot-gt-viescore-lt-a-gt-got-accepted-to-acl-2024-main-see-you-in-bangkok",title:"&lt;a href=&quot;https://tiger-ai-lab.github.io/VIEScore/&quot;&gt;VIEScore&lt;/a&gt; got accepted to ACL 2024 Main! See you in Bangkok!",description:"",section:"News"},{id:"news-lt-a-href-quot-https-tiger-ai-lab-github-io-imagenhub-quot-gt-imagenhub-lt-a-gt-got-accepted-to-iclr-2024-see-you-in-vienna",title:"&lt;a href=&quot;https://tiger-ai-lab.github.io/ImagenHub/&quot;&gt;ImagenHub&lt;/a&gt; got accepted to ICLR 2024! See you in Vienna!",description:"",section:"News"},{id:"projects-abu-robocon",title:"ABU Robocon",description:"Asia Undergraduate Robotics Contest. Each year we were given 500k HKD funding to build 2 robots to play in ABU Robocon HK (Biggest inter-university robotics event in Asia &amp; HK).",section:"Projects",handler:()=>{window.location.href="/projects/proj_aburobocon/"}},{id:"projects-anyv2v",title:"AnyV2V",description:"A video editing tool to perform Video Editing with only first frame edit. Published at TMLR 2024.",section:"Projects",handler:()=>{window.location.href="/projects/proj_anyv2v/"}},{id:"projects-dreamedit",title:"DreamEdit",description:"A pipeline for controllable Subject Replacement and Subject Addition in images. Published at TMLR 2023.",section:"Projects",handler:()=>{window.location.href="/projects/proj_dreamedit/"}},{id:"projects-1stperson-bomberman",title:"1stPerson Bomberman",description:"A 3D first person bomberman game in VR and mobile.",section:"Projects",handler:()=>{window.location.href="/projects/proj_eie3360/"}},{id:"projects-genai-arena",title:"GenAI-Arena",description:"An open platform to benchmark visual generative models. Published at NeurIPS 2024.",section:"Projects",handler:()=>{window.location.href="/projects/proj_genaiarena/"}},{id:"projects-imagenhub",title:"ImagenHub",description:"A one-stop library to standardize the inference and evaluation of all the conditional image generation models. Published at ICLR 2024.",section:"Projects",handler:()=>{window.location.href="/projects/proj_imagenhub/"}},{id:"projects-imagenmuseum",title:"ImagenMuseum",description:"A visualization page to showcase the generated images of image generation models.",section:"Projects",handler:()=>{window.location.href="/projects/proj_imagenmuseum/"}},{id:"projects-intelligent-painter",title:"Intelligent-Painter",description:"Paint a picture with a few expected objects, and generate the whole scene without text guidance. Published at ICIP 2023.",section:"Projects",handler:()=>{window.location.href="/projects/proj_ipainter/"}},{id:"projects-paper-warfare",title:"Paper Warfare",description:"A game inspired by collective primary school memories of Hong Kong teentagers playing paper-folded aircraft back in their childhood.",section:"Projects",handler:()=>{window.location.href="/projects/proj_sd3985/"}},{id:"projects-visualnovel-background-generator",title:"VisualNovel Background Generator",description:"A finetuned Stable Diffusion model to generate visual novel backgrounds.",section:"Projects",handler:()=>{window.location.href="/projects/proj_t2cgbg/"}},{id:"projects-renderer-toy",title:"Renderer Toy",description:"A Rasterization+Raytracing renderer toy built with only C++. Implemented TeamFortress2 video game shader.",section:"Projects",handler:()=>{window.location.href="/projects/proj_tf2shader/"}},{id:"projects-theoremexplainagent",title:"TheoremExplainAgent",description:"AI agent to explain theorems by generating 5 minutes long Manim Videos.",section:"Projects",handler:()=>{window.location.href="/projects/proj_theoremexplainagent/"}},{id:"projects-videogenhub",title:"VideoGenHub",description:"A one-stop library to standardize the inference for all the video generation models.",section:"Projects",handler:()=>{window.location.href="/projects/proj_videogenhub/"}},{id:"projects-viescore",title:"VIEScore",description:"A comprehensive evaluation metric for image generation and editing tasks. Published at ACL 2024.",section:"Projects",handler:()=>{window.location.href="/projects/proj_viescore/"}},{id:"projects-white-box-cartoonization-gan",title:"White-box Cartoonization GAN",description:"A pytorch implementation of Cartoon-based GAN.",section:"Projects",handler:()=>{window.location.href="/projects/proj_wbcartoonization/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6D%33%6B%75@%75%77%61%74%65%72%6C%6F%6F.%63%61","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=oCFgVhUAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/vinesmsuic","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/max-ku-650571172","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/vinesmsuic","_blank")}},{id:"socials-youtube",title:"YouTube",section:"Socials",handler:()=>{window.open("https://youtube.com/@wing-fungku9953","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js"></script> </body> </html>