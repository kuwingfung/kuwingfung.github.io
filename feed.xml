<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://kuwingfung.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://kuwingfung.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-11-21T19:32:49+00:00</updated><id>https://kuwingfung.github.io/feed.xml</id><title type="html">Max W.F. Ku</title><subtitle>Academic page of Max Ku. </subtitle><entry><title type="html">Paper Review - Modality Gap and Alignment in Multi-modal Contrastive Learning</title><link href="https://kuwingfung.github.io/blog/2024/paper-review-modality-gap-and-alignment-in-multi-modal-contrastive-learning/" rel="alternate" type="text/html" title="Paper Review - Modality Gap and Alignment in Multi-modal Contrastive Learning"/><published>2024-10-15T17:12:01+00:00</published><updated>2024-10-15T17:12:01+00:00</updated><id>https://kuwingfung.github.io/blog/2024/paper-review---modality-gap-and-alignment-in-multi-modal-contrastive-learning</id><content type="html" xml:base="https://kuwingfung.github.io/blog/2024/paper-review-modality-gap-and-alignment-in-multi-modal-contrastive-learning/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Contrastive learning is a popular self-supervised learning technique that has shown remarkable success in training deep neural networks. The core idea behind contrastive learning is to learn representations that are not only discriminative but also invariant to various transformations. This is achieved by contrasting positive and negative samples in the embedding space.]]></summary></entry><entry><title type="html">Notes on Score-Based Generative Models</title><link href="https://kuwingfung.github.io/blog/2024/notes-on-score-based-generative-models/" rel="alternate" type="text/html" title="Notes on Score-Based Generative Models"/><published>2024-10-12T06:59:23+00:00</published><updated>2024-10-12T06:59:23+00:00</updated><id>https://kuwingfung.github.io/blog/2024/notes-on-score-based-generative-models</id><content type="html" xml:base="https://kuwingfung.github.io/blog/2024/notes-on-score-based-generative-models/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[My personal notes for studying diffusion models. Watching Dome's youtube video to learn.]]></summary></entry><entry><title type="html">Paper Review - PDAE, DisDiff and InfoDiffusion</title><link href="https://kuwingfung.github.io/blog/2024/paper-review-pdae-disdiff-and-infodiffusion/" rel="alternate" type="text/html" title="Paper Review - PDAE, DisDiff and InfoDiffusion"/><published>2024-08-26T05:56:59+00:00</published><updated>2024-08-26T05:56:59+00:00</updated><id>https://kuwingfung.github.io/blog/2024/paper-review---pdae-disdiff-and-infodiffusion</id><content type="html" xml:base="https://kuwingfung.github.io/blog/2024/paper-review-pdae-disdiff-and-infodiffusion/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Literature review on Unsupervised Representation Learning in diffusion models.]]></summary></entry><entry><title type="html">Paper Review - ColorPeel</title><link href="https://kuwingfung.github.io/blog/2024/paper-review-colorpeel/" rel="alternate" type="text/html" title="Paper Review - ColorPeel"/><published>2024-07-16T16:56:41+00:00</published><updated>2024-07-16T16:56:41+00:00</updated><id>https://kuwingfung.github.io/blog/2024/paper-review---colorpeel</id><content type="html" xml:base="https://kuwingfung.github.io/blog/2024/paper-review-colorpeel/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[An interesting paper from ECCV2024. It talks about the color and shape disentanglement on Text-to-Image models. The solution is simple yet effective.]]></summary></entry><entry><title type="html">Paper Review - Disentangled Contrastive Learning on Graphs</title><link href="https://kuwingfung.github.io/blog/2024/paper-review-disentangled-contrastive-learning-on-graphs/" rel="alternate" type="text/html" title="Paper Review - Disentangled Contrastive Learning on Graphs"/><published>2024-07-16T16:51:51+00:00</published><updated>2024-07-16T16:51:51+00:00</updated><id>https://kuwingfung.github.io/blog/2024/paper-review---disentangled-contrastive-learning-on-graphs</id><content type="html" xml:base="https://kuwingfung.github.io/blog/2024/paper-review-disentangled-contrastive-learning-on-graphs/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Revisit Contrastive learning Contrastive learning is an instance-wise discriminative approach that aims at making]]></summary></entry><entry><title type="html">Paper Review - T-Cell Receptor meets VAE</title><link href="https://kuwingfung.github.io/blog/2024/paper-review-t-cell-receptor-meets-vae/" rel="alternate" type="text/html" title="Paper Review - T-Cell Receptor meets VAE"/><published>2024-07-07T05:34:32+00:00</published><updated>2024-07-07T05:34:32+00:00</updated><id>https://kuwingfung.github.io/blog/2024/paper-review---t-cell-receptor-meets-vae</id><content type="html" xml:base="https://kuwingfung.github.io/blog/2024/paper-review-t-cell-receptor-meets-vae/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[T-cell receptors (TCR) bind to certain antigens found on abnormal cells. We can perform TCR engineering with VAEs.]]></summary></entry><entry><title type="html">Paper Review - Evolutionary Scale Modeling</title><link href="https://kuwingfung.github.io/blog/2024/paper-review-evolutionary-scale-modeling/" rel="alternate" type="text/html" title="Paper Review - Evolutionary Scale Modeling"/><published>2024-06-28T07:30:26+00:00</published><updated>2024-06-28T07:30:26+00:00</updated><id>https://kuwingfung.github.io/blog/2024/paper-review---evolutionary-scale-modeling</id><content type="html" xml:base="https://kuwingfung.github.io/blog/2024/paper-review-evolutionary-scale-modeling/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[ESM-1 and ESM-2 refer to protein language models developed by Facebook AI Research under the project name Evolutionary Scale Modeling (ESM). These models are designed to understand and predict various properties of proteins by leveraging the principles of language modeling in a manner similar to how models like GPT-3 process natural language.]]></summary></entry><entry><title type="html">Paper Review - AlphaFold2 and AlphaFold3</title><link href="https://kuwingfung.github.io/blog/2024/paper-review-alphafold2-and-alphafold3/" rel="alternate" type="text/html" title="Paper Review - AlphaFold2 and AlphaFold3"/><published>2024-06-19T10:13:03+00:00</published><updated>2024-06-19T10:13:03+00:00</updated><id>https://kuwingfung.github.io/blog/2024/paper-review---alphafold2-and-alphafold3</id><content type="html" xml:base="https://kuwingfung.github.io/blog/2024/paper-review-alphafold2-and-alphafold3/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Let's try to figure out whats inside AlphaFold! AlphaFold can accurately predict structures of biomolecular interactions.]]></summary></entry><entry><title type="html">Paper Review - ProteinDT</title><link href="https://kuwingfung.github.io/blog/2024/paper-review-proteindt/" rel="alternate" type="text/html" title="Paper Review - ProteinDT"/><published>2024-06-18T10:13:03+00:00</published><updated>2024-06-18T10:13:03+00:00</updated><id>https://kuwingfung.github.io/blog/2024/paper-review---proteindt</id><content type="html" xml:base="https://kuwingfung.github.io/blog/2024/paper-review-proteindt/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[ProteinDT is A Opensource Text-guided Protein Design Framework that uses Contrastive learning to align the two modalities.]]></summary></entry><entry><title type="html">Paper Review - Axial Transformer and MSA Transformer</title><link href="https://kuwingfung.github.io/blog/2024/paper-review-axial-transformer-and-msa-transformer/" rel="alternate" type="text/html" title="Paper Review - Axial Transformer and MSA Transformer"/><published>2024-06-17T08:49:25+00:00</published><updated>2024-06-17T08:49:25+00:00</updated><id>https://kuwingfung.github.io/blog/2024/paper-review---axial-transformer-and-msa-transformer</id><content type="html" xml:base="https://kuwingfung.github.io/blog/2024/paper-review-axial-transformer-and-msa-transformer/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[The Axial Transformer is aimed at managing the complexity of high-dimensional data like images and videos, while The MSA transformer is focused on biological sequences and their alignments.]]></summary></entry></feed>