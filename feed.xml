<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://kuwingfung.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://kuwingfung.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-07-21T01:23:37+00:00</updated><id>https://kuwingfung.github.io/feed.xml</id><title type="html">Max W.F. Ku</title><subtitle>Academic page of Max Ku. </subtitle><entry><title type="html">Paper Review - Preference Learning (WIP)</title><link href="https://kuwingfung.github.io/blog/2025/paper-review-preference-learning-wip/" rel="alternate" type="text/html" title="Paper Review - Preference Learning (WIP)"/><published>2025-07-19T06:28:52+00:00</published><updated>2025-07-19T06:28:52+00:00</updated><id>https://kuwingfung.github.io/blog/2025/paper-review---preference-learning-wip</id><content type="html" xml:base="https://kuwingfung.github.io/blog/2025/paper-review-preference-learning-wip/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Quick notes on Ranking Preference Optimization, covering Bradley–Terry (BT) Model and Plackett–Luce (PL) Model.]]></summary></entry><entry><title type="html">Paper Review - FramePack and Anti-drifting Sampling</title><link href="https://kuwingfung.github.io/blog/2025/paper-review-framepack-and-anti-drifting-sampling/" rel="alternate" type="text/html" title="Paper Review - FramePack and Anti-drifting Sampling"/><published>2025-04-24T04:54:35+00:00</published><updated>2025-04-24T04:54:35+00:00</updated><id>https://kuwingfung.github.io/blog/2025/paper-review---framepack-and-anti-drifting-sampling</id><content type="html" xml:base="https://kuwingfung.github.io/blog/2025/paper-review-framepack-and-anti-drifting-sampling/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Make video diffusion practical! Quick notes on Framepack paper.]]></summary></entry><entry><title type="html">Paper Review - Pinal for De novo Protein Design</title><link href="https://kuwingfung.github.io/blog/2025/paper-review-pinal-for-de-novo-protein-design/" rel="alternate" type="text/html" title="Paper Review - Pinal for De novo Protein Design"/><published>2025-03-17T16:07:45+00:00</published><updated>2025-03-17T16:07:45+00:00</updated><id>https://kuwingfung.github.io/blog/2025/paper-review---pinal-for-de-novo-protein-design</id><content type="html" xml:base="https://kuwingfung.github.io/blog/2025/paper-review-pinal-for-de-novo-protein-design/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Quick notes on Pinal]]></summary></entry><entry><title type="html">Paper Review - Audio-Visual Related Research (WIP)</title><link href="https://kuwingfung.github.io/blog/2025/paper-review-audio-visual-related-research-wip/" rel="alternate" type="text/html" title="Paper Review - Audio-Visual Related Research (WIP)"/><published>2025-02-25T08:01:02+00:00</published><updated>2025-02-25T08:01:02+00:00</updated><id>https://kuwingfung.github.io/blog/2025/paper-review---audio-visual-related-research-wip</id><content type="html" xml:base="https://kuwingfung.github.io/blog/2025/paper-review-audio-visual-related-research-wip/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[A survey on Audio-Visual Research. Encrypted with password.]]></summary></entry><entry><title type="html">Implementing RAG for Code Library Documentation</title><link href="https://kuwingfung.github.io/blog/2025/implementing-rag-for-code-library-documentation/" rel="alternate" type="text/html" title="Implementing RAG for Code Library Documentation"/><published>2025-01-14T05:04:57+00:00</published><updated>2025-01-14T05:04:57+00:00</updated><id>https://kuwingfung.github.io/blog/2025/implementing-rag-for-code-library-documentation</id><content type="html" xml:base="https://kuwingfung.github.io/blog/2025/implementing-rag-for-code-library-documentation/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[I tried to implement RAG for Code Library Documentation. This note help me to remind the important steps in setting up a RAG.]]></summary></entry><entry><title type="html">Evaluating Protein Transfer Learning with TAPE</title><link href="https://kuwingfung.github.io/blog/2025/evaluating-protein-transfer-learning-with-tape/" rel="alternate" type="text/html" title="Evaluating Protein Transfer Learning with TAPE"/><published>2025-01-08T06:37:00+00:00</published><updated>2025-01-08T06:37:00+00:00</updated><id>https://kuwingfung.github.io/blog/2025/evaluating-protein-transfer-learning-with-tape</id><content type="html" xml:base="https://kuwingfung.github.io/blog/2025/evaluating-protein-transfer-learning-with-tape/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Tasks Assessing Protein Embeddings (TAPE) is a benchmark designed to evaluate semi-supervised learning on protein sequences.]]></summary></entry><entry><title type="html">Paper Review - Modality Gap and Alignment in Multi-modal Contrastive Learning</title><link href="https://kuwingfung.github.io/blog/2024/paper-review-modality-gap-and-alignment-in-multi-modal-contrastive-learning/" rel="alternate" type="text/html" title="Paper Review - Modality Gap and Alignment in Multi-modal Contrastive Learning"/><published>2024-10-15T17:12:01+00:00</published><updated>2024-10-15T17:12:01+00:00</updated><id>https://kuwingfung.github.io/blog/2024/paper-review---modality-gap-and-alignment-in-multi-modal-contrastive-learning</id><content type="html" xml:base="https://kuwingfung.github.io/blog/2024/paper-review-modality-gap-and-alignment-in-multi-modal-contrastive-learning/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Contrastive learning is a popular self-supervised learning technique that has shown remarkable success in training deep neural networks. The core idea behind contrastive learning is to learn representations that are not only discriminative but also invariant to various transformations. This is achieved by contrasting positive and negative samples in the embedding space.]]></summary></entry><entry><title type="html">Notes on Score-Based Generative Models</title><link href="https://kuwingfung.github.io/blog/2024/notes-on-score-based-generative-models/" rel="alternate" type="text/html" title="Notes on Score-Based Generative Models"/><published>2024-10-12T06:59:23+00:00</published><updated>2024-10-12T06:59:23+00:00</updated><id>https://kuwingfung.github.io/blog/2024/notes-on-score-based-generative-models</id><content type="html" xml:base="https://kuwingfung.github.io/blog/2024/notes-on-score-based-generative-models/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[My personal notes for studying diffusion models. Watching Dome's youtube video to learn.]]></summary></entry><entry><title type="html">Paper Review - PDAE, DisDiff and InfoDiffusion</title><link href="https://kuwingfung.github.io/blog/2024/paper-review-pdae-disdiff-and-infodiffusion/" rel="alternate" type="text/html" title="Paper Review - PDAE, DisDiff and InfoDiffusion"/><published>2024-08-26T05:56:59+00:00</published><updated>2024-08-26T05:56:59+00:00</updated><id>https://kuwingfung.github.io/blog/2024/paper-review---pdae-disdiff-and-infodiffusion</id><content type="html" xml:base="https://kuwingfung.github.io/blog/2024/paper-review-pdae-disdiff-and-infodiffusion/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Literature review on Unsupervised Representation Learning in diffusion models.]]></summary></entry><entry><title type="html">Paper Review - ColorPeel</title><link href="https://kuwingfung.github.io/blog/2024/paper-review-colorpeel/" rel="alternate" type="text/html" title="Paper Review - ColorPeel"/><published>2024-07-16T16:56:41+00:00</published><updated>2024-07-16T16:56:41+00:00</updated><id>https://kuwingfung.github.io/blog/2024/paper-review---colorpeel</id><content type="html" xml:base="https://kuwingfung.github.io/blog/2024/paper-review-colorpeel/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[An interesting paper from ECCV2024. It talks about the color and shape disentanglement on Text-to-Image models. The solution is simple yet effective.]]></summary></entry></feed>