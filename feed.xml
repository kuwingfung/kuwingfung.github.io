<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://kuwingfung.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://kuwingfung.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-03-15T00:14:28+00:00</updated><id>https://kuwingfung.github.io/feed.xml</id><title type="html">Max W.F. Ku</title><subtitle>Academic page of Max Ku. </subtitle><entry><title type="html">Paper Review - Audio-Visual Related Research (WIP)</title><link href="https://kuwingfung.github.io/blog/2025/paper-review-audio-visual-related-research-wip/" rel="alternate" type="text/html" title="Paper Review - Audio-Visual Related Research (WIP)"/><published>2025-02-25T08:01:02+00:00</published><updated>2025-02-25T08:01:02+00:00</updated><id>https://kuwingfung.github.io/blog/2025/paper-review---audio-visual-related-research-wip</id><content type="html" xml:base="https://kuwingfung.github.io/blog/2025/paper-review-audio-visual-related-research-wip/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[A survey on Audio-Visual Research. Encrypted with password.]]></summary></entry><entry><title type="html">Implementing RAG for Code Library Documentation</title><link href="https://kuwingfung.github.io/blog/2025/implementing-rag-for-code-library-documentation/" rel="alternate" type="text/html" title="Implementing RAG for Code Library Documentation"/><published>2025-01-14T05:04:57+00:00</published><updated>2025-01-14T05:04:57+00:00</updated><id>https://kuwingfung.github.io/blog/2025/implementing-rag-for-code-library-documentation</id><content type="html" xml:base="https://kuwingfung.github.io/blog/2025/implementing-rag-for-code-library-documentation/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[I tried to implement RAG for Code Library Documentation. This note help me to remind the important steps in setting up a RAG.]]></summary></entry><entry><title type="html">Evaluating Protein Transfer Learning with TAPE</title><link href="https://kuwingfung.github.io/blog/2025/evaluating-protein-transfer-learning-with-tape/" rel="alternate" type="text/html" title="Evaluating Protein Transfer Learning with TAPE"/><published>2025-01-08T06:37:00+00:00</published><updated>2025-01-08T06:37:00+00:00</updated><id>https://kuwingfung.github.io/blog/2025/evaluating-protein-transfer-learning-with-tape</id><content type="html" xml:base="https://kuwingfung.github.io/blog/2025/evaluating-protein-transfer-learning-with-tape/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Tasks Assessing Protein Embeddings (TAPE) is a benchmark designed to evaluate semi-supervised learning on protein sequences.]]></summary></entry><entry><title type="html">Paper Review - Modality Gap and Alignment in Multi-modal Contrastive Learning</title><link href="https://kuwingfung.github.io/blog/2024/paper-review-modality-gap-and-alignment-in-multi-modal-contrastive-learning/" rel="alternate" type="text/html" title="Paper Review - Modality Gap and Alignment in Multi-modal Contrastive Learning"/><published>2024-10-15T17:12:01+00:00</published><updated>2024-10-15T17:12:01+00:00</updated><id>https://kuwingfung.github.io/blog/2024/paper-review---modality-gap-and-alignment-in-multi-modal-contrastive-learning</id><content type="html" xml:base="https://kuwingfung.github.io/blog/2024/paper-review-modality-gap-and-alignment-in-multi-modal-contrastive-learning/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Contrastive learning is a popular self-supervised learning technique that has shown remarkable success in training deep neural networks. The core idea behind contrastive learning is to learn representations that are not only discriminative but also invariant to various transformations. This is achieved by contrasting positive and negative samples in the embedding space.]]></summary></entry><entry><title type="html">Notes on Score-Based Generative Models</title><link href="https://kuwingfung.github.io/blog/2024/notes-on-score-based-generative-models/" rel="alternate" type="text/html" title="Notes on Score-Based Generative Models"/><published>2024-10-12T06:59:23+00:00</published><updated>2024-10-12T06:59:23+00:00</updated><id>https://kuwingfung.github.io/blog/2024/notes-on-score-based-generative-models</id><content type="html" xml:base="https://kuwingfung.github.io/blog/2024/notes-on-score-based-generative-models/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[My personal notes for studying diffusion models. Watching Dome's youtube video to learn.]]></summary></entry><entry><title type="html">Paper Review - PDAE, DisDiff and InfoDiffusion</title><link href="https://kuwingfung.github.io/blog/2024/paper-review-pdae-disdiff-and-infodiffusion/" rel="alternate" type="text/html" title="Paper Review - PDAE, DisDiff and InfoDiffusion"/><published>2024-08-26T05:56:59+00:00</published><updated>2024-08-26T05:56:59+00:00</updated><id>https://kuwingfung.github.io/blog/2024/paper-review---pdae-disdiff-and-infodiffusion</id><content type="html" xml:base="https://kuwingfung.github.io/blog/2024/paper-review-pdae-disdiff-and-infodiffusion/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Literature review on Unsupervised Representation Learning in diffusion models.]]></summary></entry><entry><title type="html">Paper Review - ColorPeel</title><link href="https://kuwingfung.github.io/blog/2024/paper-review-colorpeel/" rel="alternate" type="text/html" title="Paper Review - ColorPeel"/><published>2024-07-16T16:56:41+00:00</published><updated>2024-07-16T16:56:41+00:00</updated><id>https://kuwingfung.github.io/blog/2024/paper-review---colorpeel</id><content type="html" xml:base="https://kuwingfung.github.io/blog/2024/paper-review-colorpeel/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[An interesting paper from ECCV2024. It talks about the color and shape disentanglement on Text-to-Image models. The solution is simple yet effective.]]></summary></entry><entry><title type="html">Paper Review - Disentangled Contrastive Learning on Graphs</title><link href="https://kuwingfung.github.io/blog/2024/paper-review-disentangled-contrastive-learning-on-graphs/" rel="alternate" type="text/html" title="Paper Review - Disentangled Contrastive Learning on Graphs"/><published>2024-07-16T16:51:51+00:00</published><updated>2024-07-16T16:51:51+00:00</updated><id>https://kuwingfung.github.io/blog/2024/paper-review---disentangled-contrastive-learning-on-graphs</id><content type="html" xml:base="https://kuwingfung.github.io/blog/2024/paper-review-disentangled-contrastive-learning-on-graphs/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Revisit Contrastive learning Contrastive learning is an instance-wise discriminative approach that aims at making]]></summary></entry><entry><title type="html">Paper Review - T-Cell Receptor meets VAE</title><link href="https://kuwingfung.github.io/blog/2024/paper-review-t-cell-receptor-meets-vae/" rel="alternate" type="text/html" title="Paper Review - T-Cell Receptor meets VAE"/><published>2024-07-07T05:34:32+00:00</published><updated>2024-07-07T05:34:32+00:00</updated><id>https://kuwingfung.github.io/blog/2024/paper-review---t-cell-receptor-meets-vae</id><content type="html" xml:base="https://kuwingfung.github.io/blog/2024/paper-review-t-cell-receptor-meets-vae/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[T-cell receptors (TCR) bind to certain antigens found on abnormal cells. We can perform TCR engineering with VAEs.]]></summary></entry><entry><title type="html">Paper Review - Evolutionary Scale Modeling</title><link href="https://kuwingfung.github.io/blog/2024/paper-review-evolutionary-scale-modeling/" rel="alternate" type="text/html" title="Paper Review - Evolutionary Scale Modeling"/><published>2024-06-28T07:30:26+00:00</published><updated>2024-06-28T07:30:26+00:00</updated><id>https://kuwingfung.github.io/blog/2024/paper-review---evolutionary-scale-modeling</id><content type="html" xml:base="https://kuwingfung.github.io/blog/2024/paper-review-evolutionary-scale-modeling/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[ESM-1 and ESM-2 refer to protein language models developed by Facebook AI Research under the project name Evolutionary Scale Modeling (ESM). These models are designed to understand and predict various properties of proteins by leveraging the principles of language modeling in a manner similar to how models like GPT-3 process natural language.]]></summary></entry></feed>